{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb5412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from xlwt import Workbook\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Initialize Chrome webdriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the initial URL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a Workbook to save the data\n",
    "wb = Workbook()\n",
    "ws = wb.add_sheet('data')\n",
    "ws.write(0, 0, 'Links')\n",
    "row = 1\n",
    "\n",
    "# Create a set to store unique links\n",
    "unique_links = set()\n",
    "\n",
    "for i in range(1,67):\n",
    "    driver.get(f'https://www.datacenters.com/locations?page={i}&per_page=40&query=&withProducts=false&showHidden=false&radius=0&bounds=&circleBounds=&polygonPath=')\n",
    "    time.sleep(3)\n",
    "    # Wait for the cookie pop-up and click the accept button\n",
    "    try:\n",
    "        cookie_accept_xpath = '//*[@id=\"onetrust-accept-btn-handler\"]'\n",
    "        cookie_accept_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, cookie_accept_xpath)))\n",
    "        cookie_accept_button.click()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        driver.execute_script(\"document.body.style.zoom='50%'\")\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        links = soup.find_all(class_=\"LocationsSearch__anchor__LzIa2\")\n",
    "        \n",
    "        for link in links:\n",
    "            link_text = link['href']  # Assuming 'href' contains the link\n",
    "            if link_text not in unique_links:\n",
    "                ws.write(row, 0, link_text)\n",
    "                row += 1\n",
    "                unique_links.add(link_text)\n",
    "                print(link_text)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "# Save the Excel file\n",
    "wb.save('links_output.xls')\n",
    "\n",
    "# Quit the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e1cee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b2a9e12",
   "metadata": {},
   "source": [
    "Extracting info from each link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1357fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "excel_file = 'links_output.xls'\n",
    "df = pd.read_excel(excel_file)\n",
    "urls = df['Links']\n",
    "\n",
    "# Loop through URLs and make requests\n",
    "for url in urls:\n",
    "    try:\n",
    "        response = requests.get(f'https://www.datacenters.com/{url}')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        soup=BeautifulSoup(response.content,'html.parser')\n",
    "        title=soup.find(class_='LocationProviderDetail__locationName__Cofdh')\n",
    "        print(title.text)        \n",
    "        contact=soup.find_all(class_='LocationProviderDetail__providerInfoItem__kuPAs')[0]\n",
    "        total_space=soup.find_all(class_='LocationProviderDetail__providerInfoItem__kuPAs')[1]\n",
    "        colocation_space=soup.find_all(class_='LocationProviderDetail__providerInfoItem__kuPAs')[2]\n",
    "        total_power=soup.find_all(class_='LocationProviderDetail__providerInfoItem__kuPAs')[3]\n",
    "        products=soup.find_all(class_='LocationProviderDetail__providerInfoItem__kuPAs')[4]\n",
    "        nearest_airport=soup.find_all(class_='LocationProviderDetail__providerInfoItem__kuPAs')[5]\n",
    "        print(contact.text)\n",
    "        print(total_space.text)\n",
    "        print(colocation_space.text)\n",
    "        print(total_power.text)\n",
    "        print(products.text)\n",
    "        print(nearest_airport.text)\n",
    "        certification_div=soup.find(class_='LocationProviderDetail__certificationsContainer__Nnx7C')\n",
    "        cr = ''\n",
    "        certi = certification_div.find_all('svg', attrs={'aria-labelledby': \"icon-title-Ok\"})\n",
    "        \n",
    "        for cer in certi:\n",
    "            if cer.text:\n",
    "                next_span = cer.find_next('span')\n",
    "                cr = cr+next_span.text+\",\"\n",
    "        print(cr)\n",
    "        fc=\"\"\n",
    "        facility_div=soup.find(class_='LocationProviderDetail__facilityTypesContainer__IzCyu')\n",
    "        facility=facility_div.find_all('svg', attrs={'aria-labelledby': \"icon-title-Ok\"})\n",
    "        for fac in facility:\n",
    "            if fac.text:\n",
    "                f_next_span=fac.find_next('span')\n",
    "                fc=fc+f_next_span.text+\",\"\n",
    "        print(f'Facility: {fc}')\n",
    "        \n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7adb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from xlwt import Workbook\n",
    "\n",
    "# Load the Excel file and read the URLs\n",
    "excel_file = 'links_output.xls'\n",
    "df = pd.read_excel(excel_file)\n",
    "urls = df['Links']\n",
    "\n",
    "# Initialize a Workbook to save the data\n",
    "wb = Workbook()\n",
    "ws = wb.add_sheet('data')\n",
    "ws.write(0, 0, 'Title')\n",
    "ws.write(0, 1, 'Contact')\n",
    "ws.write(0, 2, 'Total Space')\n",
    "ws.write(0, 3, 'Colocation Space')\n",
    "ws.write(0, 4, 'Total Power')\n",
    "ws.write(0, 5, 'Products')\n",
    "ws.write(0, 6, 'Nearest Airport')\n",
    "ws.write(0, 7, 'Certifications')\n",
    "ws.write(0, 8, 'Facility Types')\n",
    "row = 1\n",
    "output_excel_file = 'data_center_output.xls'\n",
    "# Loop through URLs and make requests\n",
    "for url in urls:\n",
    "    try:\n",
    "        response = requests.get(f'https://www.datacenters.com/{url}')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title = soup.find(class_='LocationProviderDetail__locationName__Cofdh')\n",
    "        contact = soup.find_all(class_='LocationProviderDetail__providerInfoItem__kuPAs')[0]\n",
    "        total_space = soup.find_all(class_='LocationProviderDetail__providerInfoItem__kuPAs')[1]\n",
    "        colocation_space = soup.find_all(class_='LocationProviderDetail__providerInfoItem__kuPAs')[2]\n",
    "        total_power = soup.find_all(class_='LocationProviderDetail__providerInfoItem__kuPAs')[3]\n",
    "        products = soup.find_all(class_='LocationProviderDetail__providerInfoItem__kuPAs')[4]\n",
    "        nearest_airport = soup.find_all(class_='LocationProviderDetail__providerInfoItem__kuPAs')[5]\n",
    "        certification_div = soup.find(class_='LocationProviderDetail__certificationsContainer__Nnx7C')\n",
    "        facility_div = soup.find(class_='LocationProviderDetail__facilityTypesContainer__IzCyu')\n",
    "        \n",
    "        # Extract certifications\n",
    "        cr = ''\n",
    "        certi = certification_div.find_all('svg', attrs={'aria-labelledby': \"icon-title-Ok\"})\n",
    "        for cer in certi:\n",
    "            if cer.text:\n",
    "                next_span = cer.find_next('span')\n",
    "                cr = cr + next_span.text + \",\"\n",
    "        \n",
    "        # Extract facility types\n",
    "        fc = \"\"\n",
    "        facility = facility_div.find_all('svg', attrs={'aria-labelledby': \"icon-title-Ok\"})\n",
    "        for fac in facility:\n",
    "            if fac.text:\n",
    "                f_next_span = fac.find_next('span')\n",
    "                fc = fc + f_next_span.text + \",\"\n",
    "        \n",
    "        # Write the extracted data to Excel\n",
    "        ws.write(row, 0, title.text)\n",
    "        ws.write(row, 1, contact.text)\n",
    "        ws.write(row, 2, total_space.text)\n",
    "        ws.write(row, 3, colocation_space.text)\n",
    "        ws.write(row, 4, total_power.text)\n",
    "        ws.write(row, 5, products.text)\n",
    "        ws.write(row, 6, nearest_airport.text)\n",
    "        ws.write(row, 7, cr)\n",
    "        ws.write(row, 8, fc)\n",
    "        print(contact.text)\n",
    "        print(total_space.text)\n",
    "        print(colocation_space.text)\n",
    "        print(total_power.text)\n",
    "        print(products.text)\n",
    "        print(nearest_airport.text)\n",
    "        print(cr)\n",
    "        \n",
    "        print(f'Facility: {fc}')\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    row += 1\n",
    "    wb.save(output_excel_file)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print(\"Data extraction and saving completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab84d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
